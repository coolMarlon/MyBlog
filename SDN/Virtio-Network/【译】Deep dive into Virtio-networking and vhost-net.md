原文地址：[https://www.redhat.com/en/blog/deep-dive-virtio-networking-and-vhost-net](https://app.yinxiang.com/OutboundRedirect.action?dest=https%3A%2F%2Fwww.redhat.com%2Fen%2Fblog%2Fdeep-dive-virtio-networking-and-vhost-net)

注：一开始确实尝试全部自己翻译的，但是文章实在是太长了，翻译不了了，索性用谷歌翻译了。实际发现，google翻译确实比我人工翻译的好o(╥﹏╥)o。所以你可能会发现前面的一些翻译比较奇怪。

在这一篇文章中，我们将解释[介绍](https://app.yinxiang.com/OutboundRedirect.action?dest=https%3A%2F%2Fwww.redhat.com%2Fen%2Fblog%2Fintroduction-virtio-networking-and-vhost-net)中描述vhost-net架构，从技术角度阐明一切如何协同工作。这是向您介绍 virtio-networking 领域的系列博客的一部分，它将虚拟化世界和网络世界结合在一起。

这篇博客面向对于理解前一篇博客中描述的vhost-net/virtio-net架构有兴趣的架构师和开发者。

我们将会描述hypervisor如何排列不同规范标准的virtio组件和共享内存区域，QEMU如何仿真一个网络设备以及 为了管理和在设备间通讯，guest如何使用virtio规范来实现虚拟化驱动。

再介绍QEMU virtio架构之后，我们将会分析I/O 瓶颈和限制。我们将使用host的内核来克服这些缺陷，达到概述文章（链接）中介绍的 vhost-net 架构。

最后但同样重要的是，我们将展示如何连接虚机和外部世界，超出它运行的主机，使用OpenVirtual Switch(OVS)一个开源的虚拟化的，支持SDN的分布式交换机。

看完这篇文章的最后，你将理解vhost-net/virtio-net架构如何工作；它的每一个组件的功能以及报文是如何发送和接收的。

## **之前的一些概念**

在这一节，为了更好地理解这篇博客，我们将简要介绍一些你需要知道的概念。对于精通这件事的人来说，它可能看起来很基础，但它将提供一个共同的基础。

### **网络**

让我们从基础开始。 物理 NIC（网络接口卡）是允许主机连接到外部世界的硬件（真实）组件。 它可以执行一些卸载，例如在 NIC 而不是 CPU 中执行校验和计算、分段卸载（将较大的数据分段为小块，如以太网 MTU 大小）或大型接收卸载（将许多接收到的数据包的数据加入仅 一个用于 CPU 的角度）。

另一方面，我们有 tun/tap 设备，用户空间应用程序可以用来交换数据包的虚拟点对点网络设备。 当交换的数据为第 2 层（以太网帧）时，该设备称为 tap 设备，如果交换的数据为第 3 层（IP 数据包），则称为 tun 设备。

当 tun 内核模块被加载时，它会创建一个特殊的设备 /dev/net/tun。 一个进程可以创建一个 tap 设备，打开它并向它发送特殊的 ioctl 命令。 新的 tap 设备在 /dev 文件系统中有一个名称，另一个进程可以打开它，发送和接收以太网帧。

### **IPC 系统编程**

Unix 套接字是一种在同一台机器上以有效方式进行进程间通信 (IPC) 的方法。 在这篇文章中，通信服务器将 Unix 套接字绑定到文件系统中的路径，因此客户端可以使用该路径连接到它。 从那一刻起，进程可以交换消息。 请注意，unix 套接字也可用于在进程之间交换文件描述符。

eventfd 是一种更轻松的执行 IPC 的方式。 虽然 Unix 套接字允许发送和接收任何类型的数据，但 eventfd 只是一个整数，生产者可以更改，消费者可以轮询和读取。 这使得它们更适合作为等待/通知机制，而不是信息传递。

这两个 IPC 系统都为通信中的每个进程公开一个文件描述符。 fcntl 调用对该文件描述符执行不同的操作，使它们成为非阻塞的（因此，如果没有可读取的内容，则读取操作会立即返回）。 ioctl 调用遵循相同的模式，但实现特定于设备的操作，例如发送命令。

共享内存是我们将在这里介绍的最后一种 IPC 方法。 它不是提供一个通道来通信两个进程，而是让一些进程的内存区域指向同一个内存页面，因此一个进程写入它的更改会影响另一个进程的后续读取。

### **QEMU和设备模拟**

QEMU 是一个托管虚拟机模拟器，它为guest机器提供一组不同的硬件和设备模型。 对于主机，qemu 表现为标准 Linux 调度程序调度的常规进程，具有自己的进程内存。 在这个过程中，QEMU 分配了一个虚拟机视为物理的内存区域，并执行虚拟机的 CPU 指令。

要在裸机硬件（例如存储或网络）上执行 I/O，CPU 必须与执行特殊指令和访问特定内存区域（例如设备映射到的内存区域）的物理设备进行交互。

当guest访问这些内存区域时，控制权返回给 QEMU，QEMU 以对guest透明的方式执行设备仿真。

**KVM** 基于内核的虚拟机 (KVM) 是一种内置于 Linux 中的开源虚拟化技术。 它为虚拟化软件提供硬件辅助，利用内置CPU虚拟化技术减少虚拟化开销（缓存、I/O、内存），提高安全性。

使用 KVM，QEMU 可以创建一个虚拟机，该虚拟机具有处理器识别的虚拟 CPU (vCPU)，运行原生速度指令。 当 特殊指令到达KVM 时，例如与设备交互或特殊内存区域的指令，vCPU 会暂停并通知 QEMU 暂停原因，从而允许管理程序对该事件做出反应。

在常规 KVM 操作中，管理程序打开设备 /dev/kvm，并使用 ioctl 调用与其通信以创建 VM、添加 CPU、添加内存（由 qemu 分配，但从虚拟机的角度来看是物理的）、发送 CPU 中断（作为外部设备将发送）等。例如，其中一个 ioctl 运行实际的 KVM vCPU，阻塞 QEMU 并使 vCPU 运行，直到它找到需要硬件帮助的指令。 在那一刻，ioctl 返回（这称为 vmexit）并且 QEMU 知道退出的原因（例如，违规指令）。

对于特殊的内存区域，KVM 遵循类似的方法，将内存区域标记为只读或根本不映射它们，从而以 KVM_EXIT_MMIO 原因导致 vmexit。

## **Virtio 规范**

### **virtio规范：设备和驱动**

Virtio 是虚拟机数据 I/O 通信的开放规范，为虚拟设备提供了一种直接、高效、标准和可扩展的机制，而不是限制与每一个环节和操作系统的机制。 它利用guest可以与host共享内存以进行 I/O 来实现这一点。

virtio 规范基于两个元素：设备和驱动程序。 在典型的实现中，管理程序通过多种传输方法将 virtio 设备暴露给guest。 根据设计，它们对于虚拟机中的guest来说就像物理设备。 最常见的传输方法是 PCI 或 PCIe 总线。 但是，该设备可以在某个预定义的guset的内存地址（MMIO 传输）处可用。 这些设备可以是完全虚拟的，没有物理对应物或物理设备暴露兼容接口。

暴露virtio 设备的典型（也是最简单的）方法是通过 PCI 端口，因为我们可以利用 PCI 是 QEMU 和 Linux 驱动程序中成熟且支持良好的协议这一事实。真正的 PCI 硬件使用特定的物理内存地址范围（即，驱动程序可以通过访问该内存范围来读取或写入设备的寄存器）和/或特殊的处理器指令来公开其配置空间。在 VM 世界中，管理程序捕获对该内存范围的访问并执行设备模拟，公开真实机器将具有的相同内存布局并提供相同的响应。 virtio 规范还定义了其 PCI 配置空间的布局，因此实现它很简单。 当guest启动并使用 PCI/PCIe 自动发现机制时，virtio 设备使用 PCI 供应商 ID 和它们的 PCI 设备 ID 来标识自己。guest的内核使用这些标识符来知道哪个驱动程序必须处理设备。特别是，linux 内核已经包含了 virtio 驱动程序。

virtio 驱动程序必须能够分配管理程序和设备都可以访问的内存区域以进行读写，即通过内存共享。 我们将数据平面称为使用这些内存区域的数据通信部分，而将控制平面称为设置它们的过程。 我们将在以后的文章中提供有关 virtio 协议实现和内存布局的更多详细信息。 virtio 内核驱动程序共享一个通用的传输特定接口（例如：virtio-pci），由实际传输和设备实现（例如 virtio-net 或 virtio-scsi）使用。

**Virtio specification: virtqueues**

Virtqueue 是 virtio 设备上批量数据传输的机制。 每个设备可以有零个或多个虚拟队列（[链接](https://app.yinxiang.com/OutboundRedirect.action?dest=https%3A%2F%2Fdocs.oasis-open.org%2Fvirtio%2Fvirtio%2Fv1.1%2Fcs01%2Fvirtio-v1.1-cs01.html%23x1-230005)）。 它由一个由guest分配的缓冲区组成，主机通过读取它们或写入它们来与之交互。 此外，virtio 规范还定义了双向通知：

- 可用缓冲区通知：驱动程序使用它来表示有缓冲区可供设备处理
- 已用缓冲区通知：设备使用它来表示它已完成处理某些缓冲区。

在 PCI 情况下，guest 通过写入特定内存地址来发送可用缓冲区通知，而设备（在本例中为 QEMU）使用 vCPU 中断发送已使用缓冲区通知。 virtio 规范还允许动态启用或禁用通知。 这样，设备和驱动程序可以批量缓冲区通知，甚至主动轮询虚拟队列中的新缓冲区（忙轮询）。 这种方法更适合高流量率。

总之，virtio 驱动接口暴露了：

- 设备的功能位（设备和访客必须协商）
- 状态位
- 配置空间（包含设备特定信息，如 MAC 地址）
- 通知系统（配置更改、缓冲区可用、缓冲区已使用）
- 零个或多个 virtqueue
- 将特定接口传输到设备

**Networking with virtio: qemu implementation**

![img](https://picx.zhimg.com/80/v2-4c77bad11064b9b3e17b47cb1ae4103a_720w.jpeg?source=d16d100b)



编辑切换为居中

添加图片注释，不超过 140 字（可选）

图1 :virtio-net on qemu

virtio 网络设备是一个虚拟以太网卡，它支持 TX/RX 的多队列。 空缓冲区放置在 N 个虚拟队列中用于接收数据包，而输出的数据包则被放入另外 N 个虚拟队列中进行传输。 另一个 virtqueue 用于数据平面之外的驱动程序-设备通信，例如控制高级过滤功能、mac 地址等设置或活动队列的数量。 作为一个物理网卡，virtio 设备支持许多卸载等功能，并且可以让真实主机的设备来做这些。 为了发送数据包，驱动程序向设备发送一个缓冲区，其中包含元数据信息，例如数据包所需的卸载，然后是要传输的数据包帧。 驱动程序还可以将缓冲区拆分为多个收集条目，例如 它可以从数据包帧中分离元数据头。

这些缓冲区由驱动程序管理并由设备映射。 在这种情况下，设备位于管理程序的“内部”。 由于管理程序 (qemu) 可以访问所有guest的内存，因此它能够定位缓冲区并读取或写入它们。 以下流程图显示了 virtio-net 设备配置和使用 virtio-net 驱动程序发送数据包，该驱动程序通过 PCI 与 virtio-net 设备通信。 填充完要发送的数据包后，它会触发“可用缓冲区通知”，将控制权返回给 QEMU，以便它可以通过 TAP 设备发送数据包。

然后 Qemu 通知来宾缓冲区操作（读取或写入）已完成，它通过将数据放入虚拟队列并发送已使用的通知事件，触发来宾 vCPU 中的中断来完成此操作。 接收数据包的过程与发送数据包的过程类似。 唯一的区别是，在这种情况下，空缓冲区由来宾预先分配并可供设备使用，以便它可以将传入数据写入它们。

![img](https://pic1.zhimg.com/80/v2-4c77bad11064b9b3e17b47cb1ae4103a_720w.jpeg?source=d16d100b)



编辑切换为居中

添加图片注释，不超过 140 字（可选）

*图2:Qemu virtio 发送缓冲区流程图*  **Vhost 协议** **介绍** 前面的方法包含一些低效率：

- 在 virtio 驱动程序发送可用缓冲区通知后，vCPU 停止运行，控制权返回到管理程序，导致代价高昂的上下文切换。
- QEMU 附加任务/线程同步机制。
- 每个数据包的系统调用和数据副本通过点击实际发送或接收它（无批处理）。
- ioctl 发送可用缓冲区通知（vCPU 中断）。
- 我们还需要添加另一个系统调用来恢复 vCPU 执行，以及所有相关的映射切换等。

为了解决这些限制，设计了 vhost 协议。 vhost API 是一种基于消息的协议，它允许管理程序将数据平面卸载到另一个更有效地执行数据转发的组件（处理程序）。 使用此协议，master 向处理程序发送以下配置信息：

- 管理程序的内存布局。 这样，处理程序可以在管理程序的内存空间中定位虚拟队列和缓冲区。
- 一对文件描述符，用于处理程序发送和接收 virtio 规范中定义的通知。 这些文件描述符在处理程序和 KVM 之间共享，因此它们可以直接通信而无需管理程序的干预。 请注意，每个虚拟队列仍然可以动态禁用此通知。

在此过程之后，虚拟机管理程序将不再处理数据包（从虚拟队列读取或写入/从虚拟队列写入）。 相反，数据平面将完全卸载到处理程序，它现在可以直接访问 virtqueues 的内存区域，以及直接向来宾发送和接收通知。 vhost 消息可以在任何主机本地传输协议中交换，例如 Unix 套接字或字符设备，并且虚拟机管理程序可以充当服务器或客户端（在通信通道的上下文中）。 管理程序是协议的领导者，卸载设备是处理程序，它们中的任何一个都可以发送消息。 为了进一步了解该协议的好处，我们将分析基于内核的 vhost 协议实现的细节：vhost-net 内核驱动程序。

**Vhost-net** vhost-net 是一个内核驱动程序，它实现了 vhost 协议的处理程序端，以实现高效的数据平面，即数据包转发。 在这个实现中，qemu 和 vhost-net 内核驱动程序（处理程序）使用 ioctls 来交换 vhost 消息，并且使用几个称为 irqfd 和 ioeventfd 的类似 eventfd 的文件描述符来与来宾交换通知。 当加载 vhost-net 内核驱动程序时，它会在 /dev/vhost-net 上公开一个字符设备。 当 qemu 使用 vhost-net 支持启动时，它会打开它并使用多个 ioctl(2) 调用初始化 vhost-net 实例。 这些对于将管理程序进程与 vhost-net 实例相关联、准备 virtio 功能协商并将来宾物理内存映射传递给 vhost-net 驱动程序是必需的。

在初始化期间，vhost-net 内核驱动程序创建了一个名为 vhost-$pid 的内核线程，其中 $pid 是管理程序进程 pid。 该线程称为“vhost 工作线程”。 Tap 设备仍然用于与主机通信 VM，但现在工作线程处理 I/O 事件，即它轮询驱动程序通知或点击事件，并转发数据。 Qemu 分配一个 [eventfd ](http://man7.org/linux/man-pages/man2/eventfd.2.html)并将其注册到 vhost 和 KVM 以实现通知绕过。 vhost-$pid 内核线程轮询它，当客户写入特定地址时，KVM 写入它。 这种机制被命名为 ioeventfd。 这样，对特定客户内存地址的简单读/写操作不需要经过昂贵的 QEMU 进程唤醒，可以直接路由到 vhost 工作线程。 这也有异步的优点，不需要 vCPU 停止（所以不需要立即进行上下文切换）。

另一方面，qemu 分配另一个 eventfd 并将其再次注册到 KVM 和 vhost 以进行直接 vCPU 中断注入。 这种机制称为 irqfd，它允许主机中的任何进程通过写入来向来宾注入 vCPU 中断，具有相同的优点（异步、不需要立即上下文切换等）。 请注意，virtio 数据包处理后端中的此类更改对于仍然使用标准 virtio 接口的访客来说是完全透明的。 以下框图和流程图显示了从 qemu 到 vhost-net 内核驱动程序的数据路径卸载：

![img](https://picx.zhimg.com/80/v2-7077feae088417b238ac775f2f644afe_720w.png?source=d16d100b)



编辑切换为居中

添加图片注释，不超过 140 字（可选）

*图3: vhost-net 框图*

![img](https://picx.zhimg.com/80/v2-7bade08e130ce3eacf1e8f8a9b8c93f5_720w.png?source=d16d100b)



编辑切换为居中

添加图片注释，不超过 140 字（可选）

、

*图4:*vhost-net 发送缓冲区图流程

**和外部世界通讯**  guest可以使用 tap 设备与主机通信，但问题仍然存在于它如何与同一主机上的其他 VM 或主机外部的机器（例如：与互联网）进行通信 我们可以通过使用内核网络堆栈提供的任何转发或路由机制来实现这一点，例如标准的 Linux 网桥。 但是，更高级的解决方案是使用完全虚拟化的分布式托管交换机，例如开放式虚拟交换机 ([OVS](http://www.openvswitch.org/))。 正如概述文章中所述，OVS 数据路径在这种情况下作为内核模块运行，ovs-vswitchd 作为用户空间控制和管理守护进程，ovsdb-server 作为转发数据库。 如下图所示，OVS 数据路径在内核中运行，并在物理网卡和虚拟 TAP 设备之间转发数据包：

![img](https://picx.zhimg.com/80/v2-d95b56ed8a748f438e1d9b67e6167831_720w.png?source=d16d100b)



编辑切换为居中

添加图片注释，不超过 140 字（可选）

*图5：介绍OVS*

我们可以将这种情况扩展到在同一主机环境中运行的多个 VM，每个 VM 都有自己的 on qemu 进程、TAP 端口和 vhost-net 驱动程序，这有助于避免 qemu 上下文切换。

**总结** 在这篇文章中，我们展示了 virtio-net 架构的工作原理，一步一步地剖析它并解释了每个组件的功能。 我们首先通过为guest提供开放 virtio 标准的实现来解释默认 IO qemu 设备的工作原理。 然后我们继续研究访客如何使用 virtio 驱动程序与这些设备通信，以及如何发送和接收数据包、通知主机或获得通知。 然后，我们评估了 qemu 部分的数据路径需要切换进入和切换其上下文的问题。 然后我们展示了如何使用主机中的 vhost-net 内核驱动程序使用 vhost 协议从 qemu 卸载此任务。 我们还介绍了 virtio 通知如何在这种新方法中工作。

最后但并非最不重要的一点是，我们展示了如何将 VM 连接到运行它的主机之外的外部世界。 在接下来的文章中，我们将继续提供有关 vhost-net/virtio-net 架构的实践会议，以试验解决方案概述和当前技术深度博客中介绍的不同组件。 如果您出于任何原因跳过那篇（真的很有启发性！）帖子，我们将在下一篇摘要帖子中为使用 DPDK 的 vhost 协议引入一个新的用户空间处理程序。 我们将从 DPDK 和用户空间切换的角度列举它的优点，我们将使用这些概念构建第二个架构。

发布设置

添加封面

添加文章封面

图片上传格式支持 JPEG、JPG、PNG

创作声明

无声明

*

文章话题

networking



添加话题

字数：7157

Markdown 语法识别中



草稿保存中…

预览

更新